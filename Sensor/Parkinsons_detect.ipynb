{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BBDoHPZP-dvh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqfj27Wf9HoF",
    "outputId": "b9cb3746-ad0c-4926-b4e7-5be7d8e28526",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HY_model = tf.keras.models.load_model(\"Model/HY_Final_Model.h5\")\n",
    "FOG_model = tf.keras.models.load_model(\"Model/FOG_Final_Model.h5\")\n",
    "#2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0tRVFQOawIx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Physionet Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N67tA4Ingi0Z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = ['Time', 'L1' , 'L2', 'L3', 'L4', 'L5', 'L6', 'L7', 'L8', \n",
    "            'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', \n",
    "            'Total_Force_Left', 'Total_Force_Right']\n",
    "\n",
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAYQK53fsUqi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir CSV\n",
    "\n",
    "# reading given csv file \n",
    "# and creating dataframe\n",
    "for name in os.listdir('Physionet'):\n",
    "  if 'Co' in name or 'Pt' in name:\n",
    "    # print(name)\n",
    "    df = pd.read_csv('Physionet/' + name, header = None, sep='\\t')\n",
    "      \n",
    "    # adding column headings\n",
    "    df.columns = features\n",
    "      \n",
    "    # store dataframe into csv file\n",
    "    name = 'CSV/' + name.split('.')[0]+'.csv'\n",
    "    # print(name)\n",
    "    df.to_csv(name, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwSvyujwzxS5",
    "outputId": "d687d5b0-f358-4ef1-85cc-0f93e043bbb9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects Count (in demographics) =  166\n",
      "Files count =  306\n"
     ]
    }
   ],
   "source": [
    "demographics = pd.read_csv('Physionet/demographics.txt', delim_whitespace=True)\n",
    "sub_id = demographics.ID.to_list()\n",
    "sub_names = []\n",
    "for name in os.listdir('CSV'):\n",
    "  sub_name = name.split('_')[0]\n",
    "  sub_names.append(sub_name)\n",
    "\n",
    "print(\"Subjects Count (in demographics) = \", len(sub_id))\n",
    "print(\"Files count = \", len(sub_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbRiGxHRQ_bd",
    "outputId": "1902c68c-b9a9-4ccb-edc2-5b971b9ed2a3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    73\n",
       "2.0    55\n",
       "2.5    28\n",
       "3.0    10\n",
       "Name: HoehnYahr, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics.HoehnYahr\n",
    "demographics['HoehnYahr'] = demographics['HoehnYahr'].fillna(0)\n",
    "demographics.HoehnYahr.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxMtwLPxUaAH",
    "outputId": "8da38dab-7b76-4ee6-f425-492f2f913b12",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    73\n",
       "2    55\n",
       "1    28\n",
       "3    10\n",
       "Name: HoehnYahr, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics['HoehnYahr'].replace({2.5: 1}, inplace=True)\n",
    "demographics['HoehnYahr'] = demographics['HoehnYahr'].astype(int)\n",
    "demographics.HoehnYahr.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHlBADgVIHzZ",
    "outputId": "b00a3007-a492-451d-dc18-600ddbeb502d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Count Subjects =  214\n",
      "Test Count Subjects =  92\n",
      "1 GaCo01_01.csv train\n",
      "2 GaCo02_01.csv train\n",
      "3 GaCo02_02.csv train\n",
      "4 GaCo03_01.csv train\n",
      "5 GaCo03_02.csv train\n",
      "6 GaCo04_01.csv train\n",
      "7 GaCo04_02.csv train\n",
      "8 GaCo05_01.csv train\n",
      "9 GaCo05_02.csv train\n",
      "10 GaCo06_01.csv train\n",
      "11 GaCo06_02.csv train\n",
      "12 GaCo07_01.csv train\n",
      "13 GaCo07_02.csv train\n",
      "14 GaCo08_01.csv train\n",
      "15 GaCo08_02.csv train\n",
      "16 GaCo09_01.csv train\n",
      "17 GaCo09_02.csv train\n",
      "18 GaCo10_01.csv train\n",
      "19 GaCo10_02.csv train\n",
      "20 GaCo11_01.csv train\n",
      "21 GaCo12_01.csv train\n",
      "22 GaCo13_01.csv train\n",
      "23 GaCo13_02.csv train\n",
      "24 GaCo13_10.csv train\n",
      "25 GaCo14_01.csv train\n",
      "26 GaCo14_02.csv train\n",
      "27 GaCo14_10.csv train\n",
      "28 GaCo15_01.csv train\n",
      "29 GaCo15_02.csv train\n",
      "30 GaCo15_10.csv train\n",
      "31 GaCo16_01.csv train\n",
      "32 GaCo16_02.csv train\n",
      "33 GaCo16_10.csv train\n",
      "34 GaCo17_01.csv train\n",
      "35 GaCo17_02.csv train\n",
      "36 GaCo17_10.csv train\n",
      "37 GaCo22_01.csv train\n",
      "38 GaCo22_10.csv train\n",
      "39 GaPt03_01.csv train\n",
      "40 GaPt04_01.csv train\n",
      "41 GaPt05_01.csv train\n",
      "42 GaPt06_01.csv train\n",
      "43 GaPt07_01.csv train\n",
      "44 GaPt07_02.csv train\n",
      "45 GaPt08_01.csv train\n",
      "46 GaPt08_02.csv train\n",
      "47 GaPt09_01.csv train\n",
      "48 GaPt09_02.csv train\n",
      "49 GaPt12_01.csv train\n",
      "50 GaPt12_02.csv train\n",
      "51 GaPt13_01.csv train\n",
      "52 GaPt13_02.csv train\n",
      "53 GaPt13_10.csv train\n",
      "54 GaPt14_01.csv train\n",
      "55 GaPt14_02.csv train\n",
      "56 GaPt14_10.csv train\n",
      "57 GaPt15_01.csv train\n",
      "58 GaPt15_02.csv train\n",
      "59 GaPt15_10.csv train\n",
      "60 GaPt16_01.csv train\n",
      "61 GaPt16_02.csv train\n",
      "62 GaPt16_10.csv train\n",
      "63 GaPt17_01.csv train\n",
      "64 GaPt17_02.csv train\n",
      "65 GaPt17_10.csv train\n",
      "66 GaPt18_01.csv train\n",
      "67 GaPt18_02.csv train\n",
      "68 GaPt18_10.csv train\n",
      "69 GaPt19_01.csv train\n",
      "70 GaPt19_02.csv train\n",
      "71 GaPt19_10.csv train\n",
      "72 GaPt20_01.csv train\n",
      "73 GaPt20_02.csv train\n",
      "74 GaPt20_10.csv train\n",
      "75 GaPt21_01.csv train\n",
      "76 GaPt21_02.csv train\n",
      "77 GaPt21_10.csv train\n",
      "78 GaPt22_01.csv train\n",
      "79 GaPt22_02.csv train\n",
      "80 GaPt22_10.csv train\n",
      "81 GaPt23_01.csv train\n",
      "82 GaPt23_02.csv train\n",
      "83 GaPt23_10.csv train\n",
      "84 GaPt24_01.csv train\n",
      "85 GaPt24_02.csv train\n",
      "86 GaPt24_10.csv train\n",
      "87 GaPt25_01.csv train\n",
      "88 GaPt25_02.csv train\n",
      "89 GaPt25_10.csv train\n",
      "90 GaPt26_01.csv train\n",
      "91 GaPt26_02.csv train\n",
      "92 GaPt26_10.csv train\n",
      "93 GaPt27_01.csv train\n",
      "94 GaPt27_02.csv train\n",
      "95 GaPt27_10.csv train\n",
      "96 GaPt28_01.csv train\n",
      "97 GaPt28_02.csv train\n",
      "98 GaPt28_10.csv train\n",
      "99 GaPt29_01.csv train\n",
      "100 GaPt29_02.csv train\n",
      "101 GaPt29_10.csv train\n",
      "102 GaPt30_01.csv train\n",
      "103 GaPt30_02.csv train\n",
      "104 GaPt30_10.csv train\n",
      "105 GaPt31_01.csv train\n",
      "106 GaPt31_02.csv train\n",
      "107 GaPt31_10.csv train\n",
      "108 GaPt32_01.csv train\n",
      "109 GaPt32_02.csv train\n",
      "110 GaPt32_10.csv train\n",
      "111 GaPt33_01.csv train\n",
      "112 GaPt33_02.csv train\n",
      "113 GaPt33_10.csv train\n",
      "114 JuCo01_01.csv train\n",
      "115 JuCo02_01.csv train\n",
      "116 JuCo03_01.csv train\n",
      "117 JuCo04_01.csv train\n",
      "118 JuCo05_01.csv train\n",
      "119 JuCo06_01.csv train\n",
      "120 JuCo07_01.csv train\n",
      "121 JuCo08_01.csv train\n",
      "122 JuCo09_01.csv train\n",
      "123 JuCo11_01.csv train\n",
      "124 JuCo12_01.csv train\n",
      "125 JuCo13_01.csv train\n",
      "126 JuCo14_01.csv train\n",
      "127 JuCo15_01.csv train\n",
      "128 JuCo16_01.csv train\n",
      "129 JuCo17_01.csv train\n",
      "130 JuCo18_01.csv train\n",
      "131 JuCo19_01.csv train\n",
      "132 JuCo20_01.csv train\n",
      "133 JuCo21_01.csv train\n",
      "134 JuCo22_01.csv train\n",
      "135 JuCo23_01.csv train\n",
      "136 JuCo24_01.csv train\n",
      "137 JuCo25_01.csv train\n",
      "138 JuCo26_01.csv train\n",
      "139 JuPt01_01.csv train\n",
      "140 JuPt01_02.csv train\n",
      "141 JuPt01_03.csv train\n",
      "142 JuPt01_04.csv train\n",
      "143 JuPt01_05.csv train\n",
      "144 JuPt01_06.csv train\n",
      "145 JuPt02_01.csv train\n",
      "146 JuPt03_01.csv train\n",
      "147 JuPt03_02.csv train\n",
      "148 JuPt03_03.csv train\n",
      "149 JuPt03_04.csv train\n",
      "150 JuPt03_05.csv train\n",
      "151 JuPt03_06.csv train\n",
      "152 JuPt03_07.csv train\n",
      "153 JuPt04_01.csv train\n",
      "154 JuPt05_01.csv train\n",
      "155 JuPt06_01.csv train\n",
      "156 JuPt06_02.csv train\n",
      "157 JuPt06_03.csv train\n",
      "158 JuPt06_04.csv train\n",
      "159 JuPt06_05.csv train\n",
      "160 JuPt06_06.csv train\n",
      "161 JuPt06_07.csv train\n",
      "162 JuPt07_01.csv train\n",
      "163 JuPt08_01.csv train\n",
      "164 JuPt09_01.csv train\n",
      "165 JuPt09_02.csv train\n",
      "166 JuPt09_03.csv train\n",
      "167 JuPt09_04.csv train\n",
      "168 JuPt09_05.csv train\n",
      "169 JuPt10_01.csv train\n",
      "170 JuPt10_02.csv train\n",
      "171 JuPt10_03.csv train\n",
      "172 JuPt10_04.csv train\n",
      "173 JuPt10_05.csv train\n",
      "174 JuPt10_06.csv train\n",
      "175 JuPt10_07.csv train\n",
      "176 JuPt11_01.csv train\n",
      "177 JuPt11_02.csv train\n",
      "178 JuPt11_03.csv train\n",
      "179 JuPt11_04.csv train\n",
      "180 JuPt11_05.csv train\n",
      "181 JuPt11_06.csv train\n",
      "182 JuPt11_07.csv train\n",
      "183 JuPt12_01.csv train\n",
      "184 JuPt13_01.csv train\n",
      "185 JuPt14_01.csv train\n",
      "186 JuPt15_01.csv train\n",
      "187 JuPt15_02.csv train\n",
      "188 JuPt15_03.csv train\n",
      "189 JuPt15_04.csv train\n",
      "190 JuPt15_05.csv train\n",
      "191 JuPt15_06.csv train\n",
      "192 JuPt15_07.csv train\n",
      "193 JuPt16_01.csv train\n",
      "194 JuPt17_01.csv train\n",
      "195 JuPt18_01.csv train\n",
      "196 JuPt19_01.csv train\n",
      "197 JuPt20_01.csv train\n",
      "198 JuPt20_02.csv train\n",
      "199 JuPt20_03.csv train\n",
      "200 JuPt20_04.csv train\n",
      "201 JuPt20_05.csv train\n",
      "202 JuPt20_06.csv train\n",
      "203 JuPt20_07.csv train\n",
      "204 JuPt21_01.csv train\n",
      "205 JuPt21_02.csv train\n",
      "206 JuPt21_03.csv train\n",
      "207 JuPt21_04.csv train\n",
      "208 JuPt21_05.csv train\n",
      "209 JuPt21_06.csv train\n",
      "210 JuPt21_07.csv train\n",
      "211 JuPt22_01.csv train\n",
      "212 JuPt23_01.csv train\n",
      "213 JuPt23_02.csv train\n",
      "214 JuPt23_03.csv train\n",
      "215 JuPt23_04.csv test\n",
      "216 JuPt23_05.csv test\n",
      "217 JuPt23_06.csv test\n",
      "218 JuPt23_07.csv test\n",
      "219 JuPt24_01.csv test\n",
      "220 JuPt24_02.csv test\n",
      "221 JuPt25_01.csv test\n",
      "222 JuPt26_01.csv test\n",
      "223 JuPt26_03.csv test\n",
      "224 JuPt26_04.csv test\n",
      "225 JuPt26_05.csv test\n",
      "226 JuPt26_06.csv test\n",
      "227 JuPt26_07.csv test\n",
      "228 JuPt27_01.csv test\n",
      "229 JuPt28_01.csv test\n",
      "230 JuPt28_02.csv test\n",
      "231 JuPt28_03.csv test\n",
      "232 JuPt28_04.csv test\n",
      "233 JuPt28_05.csv test\n",
      "234 JuPt28_06.csv test\n",
      "235 JuPt28_07.csv test\n",
      "236 JuPt29_01.csv test\n",
      "237 JuPt29_02.csv test\n",
      "238 JuPt29_03.csv test\n",
      "239 JuPt29_04.csv test\n",
      "240 JuPt29_05.csv test\n",
      "241 JuPt29_06.csv test\n",
      "242 JuPt29_07.csv test\n",
      "243 SiCo01_01.csv test\n",
      "244 SiCo03_01.csv test\n",
      "245 SiCo04_01.csv test\n",
      "246 SiCo05_01.csv test\n",
      "247 SiCo06_01.csv test\n",
      "248 SiCo07_01.csv test\n",
      "249 SiCo08_01.csv test\n",
      "250 SiCo09_01.csv test\n",
      "251 SiCo10_01.csv test\n",
      "252 SiCo11_01.csv test\n",
      "253 SiCo12_01.csv test\n",
      "254 SiCo13_01.csv test\n",
      "255 SiCo14_01.csv test\n",
      "256 SiCo15_01.csv test\n",
      "257 SiCo16_01.csv test\n",
      "258 SiCo17_01.csv test\n",
      "259 SiCo18_01.csv test\n",
      "260 SiCo19_01.csv test\n",
      "261 SiCo20_01.csv test\n",
      "262 SiCo21_01.csv test\n",
      "263 SiCo22_01.csv test\n",
      "264 SiCo23_01.csv test\n",
      "265 SiCo24_01.csv test\n",
      "266 SiCo25_01.csv test\n",
      "267 SiCo26_01.csv test\n",
      "268 SiCo27_01.csv test\n",
      "269 SiCo28_01.csv test\n",
      "270 SiCo29_01.csv test\n",
      "271 SiCo30_01.csv test\n",
      "272 SiPt02_01.csv test\n",
      "273 SiPt04_01.csv test\n",
      "274 SiPt05_01.csv test\n",
      "275 SiPt07_01.csv test\n",
      "276 SiPt08_01.csv test\n",
      "277 SiPt09_01.csv test\n",
      "278 SiPt10_01.csv test\n",
      "279 SiPt12_01.csv test\n",
      "280 SiPt13_01.csv test\n",
      "281 SiPt14_01.csv test\n",
      "282 SiPt15_01.csv test\n",
      "283 SiPt16_01.csv test\n",
      "284 SiPt17_01.csv test\n",
      "285 SiPt18_01.csv test\n",
      "286 SiPt19_01.csv test\n",
      "287 SiPt20_01.csv test\n",
      "288 SiPt21_01.csv test\n",
      "289 SiPt22_01.csv test\n",
      "290 SiPt23_01.csv test\n",
      "291 SiPt24_01.csv test\n",
      "292 SiPt25_01.csv test\n",
      "293 SiPt27_01.csv test\n",
      "294 SiPt28_01.csv test\n",
      "295 SiPt29_01.csv test\n",
      "296 SiPt30_01.csv test\n",
      "297 SiPt31_01.csv test\n",
      "298 SiPt32_01.csv test\n",
      "299 SiPt33_01.csv test\n",
      "300 SiPt34_01.csv test\n",
      "301 SiPt35_01.csv test\n",
      "302 SiPt36_01.csv test\n",
      "303 SiPt37_01.csv test\n",
      "304 SiPt38_01.csv test\n",
      "305 SiPt39_01.csv test\n",
      "306 SiPt40_01.csv test\n"
     ]
    }
   ],
   "source": [
    "# !mkdir Final\n",
    "# !mkdir Final/train\n",
    "# !mkdir Final/test\n",
    "\n",
    "count = len(os.listdir('CSV'))\n",
    "train_count = int(70/100*count)\n",
    "test_count = count - train_count\n",
    "\n",
    "print(\"Training Count Subjects = \", train_count)\n",
    "print(\"Test Count Subjects = \", test_count)\n",
    "\n",
    "category = 'train'\n",
    "counter = 1\n",
    "\n",
    "ypath = 'Final/y_' + category + '.txt'\n",
    "yfile = open(ypath, \"a\")\n",
    "hypath = 'Final/hyscore_' + category + '.txt'\n",
    "hyfile = open(hypath, \"a\")\n",
    "\n",
    "for name in os.listdir('CSV'):\n",
    "  if counter == train_count + 1:\n",
    "    yfile.flush()\n",
    "    yfile.close()\n",
    "    hyfile.flush()\n",
    "    hyfile.close()\n",
    "    \n",
    "    category = 'test'\n",
    "    ypath = 'Final/y_' + category + '.txt'\n",
    "    yfile = open(ypath, \"a\")\n",
    "    hypath = 'Final/hyscore_' + category + '.txt'\n",
    "    hyfile = open(hypath, \"a\")\n",
    "  print(counter, name, category)\n",
    "  \n",
    "  sub_name = name.split('_')[0]\n",
    "  sub_class = demographics[demographics['ID'] == sub_name]['Group'].to_string(index=False).strip()\n",
    "  hy_class = demographics[demographics['ID'] == sub_name]['HoehnYahr'].to_string(index=False).strip()\n",
    "  sub_data = pd.read_csv('CSV/' + name)\n",
    "  features = sub_data.columns.to_list()\n",
    "\n",
    "  full_size = 100\n",
    "  overlap = 0.5\n",
    "  overlap_size = int(full_size * overlap / 2)\n",
    "  entry_size = full_size - overlap_size\n",
    "  \n",
    "  for i in range(0, sub_data.shape[0], entry_size):\n",
    "    if sub_data.shape[0] >= i + entry_size + overlap_size:\n",
    "      yfile.write(sub_class + \"\\n\")\n",
    "      hyfile.write(hy_class + \"\\n\")\n",
    "      \n",
    "      for fname in features:\n",
    "        path_name = 'Final/' + category + '/'\n",
    "        file_name = fname + '_' + category + '.txt'\n",
    "        with open(path_name + file_name, 'a') as feat_file:\n",
    "          arr = sub_data.iloc[i:i + entry_size + overlap_size, ][fname].to_numpy()\n",
    "          # print(i, len(arr), end = ' | ')\n",
    "          feat_file.write(' '.join(map(str, arr)) + \"\\n\")\n",
    "\n",
    "  counter = counter + 1\n",
    "yfile.flush()\n",
    "yfile.close()\n",
    "hyfile.flush()\n",
    "yfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "unHHUiZ2ZMiS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "\tdataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    "\n",
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_category(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = np.dstack(loaded)\n",
    "\treturn loaded\n",
    "\n",
    "\n",
    "# load a dataset category, such as train or test\n",
    "def load_dataset_category(category, prefix=''):\n",
    "\t# load all 19 files as a single array\n",
    "\tfilenames = []\n",
    "\tfor fname in features:\n",
    "\t\tfilenames.append(category + '/' + fname + '_' + category + '.txt')\n",
    "  \n",
    "\t# load input data\n",
    "\tX = load_category(filenames, prefix)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + 'y_'+ category +'.txt')\n",
    "\thy = load_file(prefix + 'hyscore_'+ category +'.txt')\n",
    "\treturn X, y, hy\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "\t# load all train\n",
    "\ttrainX, trainy, trainHY = load_dataset_category('train', prefix)\n",
    "\tprint(trainX.shape, trainy.shape, trainHY.shape)\n",
    "\t# load all test\n",
    "\ttestX, testy, testHY = load_dataset_category('test', prefix)\n",
    "\tprint(testX.shape, testy.shape, testHY.shape)\n",
    " \n",
    "\t# zero-offset class values\n",
    "\ttrainy = trainy - 1\n",
    "\ttesty = testy - 1\n",
    "\t# one hot encode y\n",
    "\ttrainy = tf.keras.utils.to_categorical(trainy)\n",
    "\ttrainHY = tf.keras.utils.to_categorical(trainHY)\n",
    "\ttesty = tf.keras.utils.to_categorical(testy)\n",
    "\ttestHY = tf.keras.utils.to_categorical(testHY)  \n",
    " \n",
    "\t# print(trainX.shape, trainy.shape, trainHY.shape, testX.shape, testy.shape, testHY.shape)\n",
    "\treturn trainX, trainy, trainHY, testX, testy, testHY\n",
    "\n",
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oALGmMo3ZOEd",
    "outputId": "342aaeb0-48de-43da-e5cc-c5253900eaa6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30104, 100, 19) (30104, 1) (30104, 1)\n",
      "(13761, 100, 19) (13761, 1) (13761, 1)\n",
      "Training Data =  (30104, 100, 19)\n",
      "Training Class =  (30104, 2)\n",
      "Training HoehnYahr Class =  (30104, 4)\n",
      "Test Data =  (13761, 100, 19)\n",
      "Test Class =  (13761, 2)\n",
      "Test HoehnYahr Class =  (13761, 3)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainy, trainHY, testX, testy, testHY = load_dataset('Final/')\n",
    "print(\"Training Data = \", trainX.shape)\n",
    "print(\"Training Class = \", trainy.shape)\n",
    "print(\"Training HoehnYahr Class = \", trainHY.shape)\n",
    "print(\"Test Data = \", testX.shape)\n",
    "print(\"Test Class = \", testy.shape)\n",
    "print(\"Test HoehnYahr Class = \", testHY.shape)\n",
    "#5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT9D5bIiarXW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Daphnet Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlcLQeFwM52N",
    "outputId": "fa7352b1-3673-44df-edb4-ebdb4b83a6e2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_var = 'annontations'\n",
    "features = ['Time', 'ankle-x', 'ankle-y', 'ankle-z', \n",
    "            'thigh-x', 'thigh-y', 'thigh-z',\n",
    "            'trunk-x', 'trunk-y', 'trunk-z', class_var]\n",
    "len(os.listdir('dataset_fog_release/dataset'))\n",
    "#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pthqxf0JiIBF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir FOG_CSV\n",
    "\n",
    "# reading given csv file /\n",
    "# and creating dataframe\n",
    "for name in os.listdir('dataset_fog_release/dataset'):\n",
    "    fog_df = pd.read_csv('dataset_fog_release/dataset/' + name, header = None, sep=' ')\n",
    "      \n",
    "    # adding column headings\n",
    "    fog_df.columns = features\n",
    "    fog_df = fog_df[fog_df.annontations != 0]  \n",
    "\n",
    "    # store dataframe into csv file\n",
    "    name = 'FOG_CSV/' + name.split('.')[0]+'.csv'\n",
    "    # print(name)\n",
    "    fog_df.to_csv(name, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72npFjr6mbWt",
    "outputId": "73c83846-0db4-4ff5-c8a8-e174de473831",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Count Subjects =  14\n",
      "Test Count Subjects =  3\n",
      "1 S01R01.csv train\n",
      "2 S01R02.csv train\n",
      "3 S02R01.csv train\n",
      "4 S02R02.csv train\n",
      "5 S03R01.csv train\n",
      "6 S03R02.csv train\n",
      "7 S03R03.csv train\n",
      "8 S04R01.csv train\n",
      "9 S05R01.csv train\n",
      "10 S05R02.csv train\n",
      "11 S06R01.csv train\n",
      "12 S06R02.csv train\n",
      "13 S07R01.csv train\n",
      "14 S07R02.csv train\n",
      "15 S08R01.csv test\n",
      "16 S09R01.csv test\n",
      "17 S10R01.csv test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "count = len(os.listdir('CSV_D'))\n",
    "\n",
    "train_count = int(85/100*count)\n",
    "test_count = count - train_count\n",
    "\n",
    "print(\"Training Count Subjects = \", train_count)\n",
    "print(\"Test Count Subjects = \", test_count)\n",
    "\n",
    "category = 'train'\n",
    "counter = 1\n",
    "\n",
    "ypath = 'Final_D/y_' + category + '.txt'\n",
    "yfile = open(ypath, \"a\")\n",
    "\n",
    "for name in sorted(os.listdir('CSV_D')):\n",
    "  if counter == train_count + 1:\n",
    "    yfile.flush()\n",
    "    yfile.close()\n",
    "    \n",
    "    category = 'test'    \n",
    "    ypath = 'Final_D/y_' + category + '.txt'\n",
    "    yfile = open(ypath, \"a\")\n",
    "  print(counter, name, category)\n",
    "  \n",
    "  sub_name = name.split('R')[0]\n",
    "  sub_data = pd.read_csv('FOG_CSV/' + name)\n",
    "  features = sub_data.columns.to_list()\n",
    "  \n",
    "  full_size = 128\n",
    "  overlap = 0.5\n",
    "  overlap_size = int(full_size * overlap / 2)\n",
    "  entry_size = full_size - overlap_size\n",
    "\n",
    "  for i in range(0, sub_data.shape[0], entry_size):\n",
    "    if sub_data.shape[0] >= i + entry_size + overlap_size:\n",
    "      sub_classes = sub_data.iloc[i:i+entry_size+overlap_size, ][class_var].to_list()\n",
    "      sub_class = max(sub_classes, key = sub_classes.count)\n",
    "      yfile.write(str(sub_class) + \"\\n\")\n",
    " \n",
    "      for fname in features:\n",
    "        if fname != 'Time' and fname != class_var:\n",
    "          path_name = 'Final_D/' + category + '/'\n",
    "          file_name = fname + '_' + category + '.txt'\n",
    "          with open(path_name + file_name, 'a') as feat_file:\n",
    "            arr = sub_data.iloc[i:i+entry_size+overlap_size, ][fname].to_numpy()\n",
    "            # print(i, len(arr), end = ' | ')\n",
    "            feat_file.write(' '.join(map(str, arr)) + \"\\n\")\n",
    "  counter = counter + 1\n",
    "yfile.flush()\n",
    "yfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "02lrGh-Q1pv3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_file_daphnet(filepath):\n",
    "\tdataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    "\n",
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_category_daphnet(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file_daphnet(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = np.dstack(loaded)\n",
    "\treturn loaded\n",
    "\n",
    "# load a dataset category, such as train or test\n",
    "def load_dataset_category_daphnet(category, prefix=''):\n",
    "\t# load all 10 files as a single array\n",
    "  filenames = []\n",
    "  for fname in features:\n",
    "    if fname != 'Time' and fname != class_var:\n",
    "      filenames.append(category + '/' + fname + '_' + category + '.txt')\n",
    "  \n",
    "  # load input data\n",
    "  X = load_category_daphnet(filenames, prefix)\n",
    "  # load class output\n",
    "  y = load_file_daphnet(prefix + 'y_'+ category +'.txt')\n",
    "  return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset_daphnet(prefix=''):\n",
    "  # load all train\n",
    "  trainX, trainy = load_dataset_category_daphnet('train', prefix)\n",
    "  print(trainX.shape, trainy.shape)\n",
    "  # load all test\n",
    "  testX, testy = load_dataset_category_daphnet('test', prefix)\n",
    "  print(testX.shape, testy.shape)\n",
    "\n",
    "  # zero-offset class values\n",
    "  trainy = trainy - 1\n",
    "  testy = testy - 1\n",
    "\n",
    "  neg, pos = np.bincount(trainy.reshape(1, trainy.shape[0])[0])\n",
    "  total = neg + pos\n",
    "  print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))\n",
    "  # one hot encode y\n",
    "  trainy = tf.keras.utils.to_categorical(trainy)\n",
    "  testy = tf.keras.utils.to_categorical(testy)\n",
    "\n",
    "  # print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "  return trainX, trainy, testX, testy, neg, pos, total\n",
    "\n",
    "#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uTFlFGZG2ZcQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7932, 128, 9) (7932, 1)\n",
      "(3158, 128, 9) (3158, 1)\n",
      "Examples:\n",
      "    Total: 7932\n",
      "    Positive: 717 (9.04% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fog_trainX, fog_trainy, fog_testX, fog_testy, neg, pos, total = load_dataset_daphnet('Final_D/')\n",
    "#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMPRIAoFiH_n",
    "outputId": "1a289999-d70b-4a48-a901-2616b2c457cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data =  (7932, 128, 9)\n",
      "Training Class =  (7932, 2)\n",
      "Test Data =  (3158, 128, 9)\n",
      "Test Class =  (3158, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data = \", fog_trainX.shape)\n",
    "print(\"Training Class = \", fog_trainy.shape)\n",
    "print(\"Test Data = \", fog_testX.shape)\n",
    "print(\"Test Class = \", fog_testy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8qYq-1Zd_zR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Final Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PA-OxLdJeBr9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def final_prediction(HY_model ,FOG_model, pd_data,fog_data, fog_class):\n",
    "  # pd_data - 100 x 19\n",
    "  # fog_data - 128 x 9\n",
    "  hy_pred = np.argmax(HY_model.predict(np.expand_dims(pd_data, axis=0)))\n",
    "  fog_pred = np.argmax(FOG_model.predict(np.expand_dims(fog_data, axis=0)))\n",
    "\n",
    "  print(\"ORIGINAL CLASS\")\n",
    "  if np.argmax(fog_class) == 0:\n",
    "      print(\"No Freeze\")\n",
    "  else:\n",
    "    print(\"Freeze\")\n",
    "\n",
    "  print(\"-\"*20)\n",
    "\n",
    "  '''\n",
    "    No Freeze - 0\n",
    "    Freeze - 1\n",
    "  '''\n",
    "  \n",
    "  print(\"PREDICTION\")\n",
    "  if fog_pred == 1:\n",
    "    print(\"Freezing of GAIT\")\n",
    "  else:\n",
    "    print(\"No Freezing of GAIT\")\n",
    "\n",
    "\n",
    "  \n",
    "  '''\n",
    "  Hoehn Yahr Scale\n",
    "    0.0\t- 0\n",
    "    2.5\t- 1\n",
    "    2.0\t- 2\n",
    "    3.0\t- 3\n",
    "  '''\n",
    "  print(\"Hoehn Yahr Scale Severity: \", end=\"\")\n",
    "  if hy_pred == 0:\n",
    "    print(\"Healthy Control\")\n",
    "  elif hy_pred == 1:\n",
    "    print(\"2.5 severity\")\n",
    "  elif hy_pred == 2:\n",
    "    print(\"2 severity\")\n",
    "  elif hy_pred == 3:\n",
    "    print(\"3 severity\")\n",
    "  \n",
    "  print(\"_\"*40, \"\\n\")\n",
    "\n",
    "#9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Random Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: Healthy Control\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2.5 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2.5 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2.5 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2.5 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2.5 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2.5 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2.5 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: Healthy Control\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2.5 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2.5 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2.5 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: Healthy Control\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n",
      "ORIGINAL CLASS\n",
      "No Freeze\n",
      "--------------------\n",
      "PREDICTION\n",
      "No Freezing of GAIT\n",
      "Hoehn Yahr Scale Severity: 2 severity\n",
      "________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(5):\n",
    "  i = random.randrange(0, min(len(testX), len(fog_testX)))\n",
    "  final_prediction(HY_model,FOG_model, testX[i], fog_testX[i], fog_testy[i])\n",
    "  #10"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Human Biomechanic Analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "2bc555d4404cb76c05abf652b46a3ff7311252ed528c07b449819f2d141a7c7f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
